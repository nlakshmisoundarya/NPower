{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYy32hqkNTtGrcmgdG3Gg8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nlakshmisoundarya/NPower/blob/main/API_Data_Wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Essential Python Libraries for Data Wrangling\n",
        "\n",
        "Essential Python Libraries for Data Wrangling\n",
        "\n",
        "\n",
        "**pandas**\n",
        "Data manipulation, filtering, grouping, merging, pivoting, missing values handling\n",
        "\n",
        "\n",
        "**numpy**\n",
        "Numerical operations, arrays, missing values, mathematical functions\n",
        "\n",
        "\n",
        "**requests**\n",
        "Fetching API data from RESTful web services\n",
        "\n",
        "\n",
        "**json**\n",
        "Parsing JSON responses from APIs and files\n",
        "\n",
        "\n",
        "**xml.etree.ElementTree**\n",
        "Parsing and extracting data from XML files\n",
        "\n",
        "\n",
        "**sqlite3**\n",
        "Working with SQLite databases directly in Python\n",
        "\n",
        "\n",
        "**sqlalchemy**\n",
        "Connecting Python with databases like MySQL, PostgreSQL, SQLite\n",
        "\n",
        "\n",
        "**schedule**\n",
        "Automating API calls and data fetching at intervals"
      ],
      "metadata": {
        "id": "jR9l6iZ4jHfK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "39uInoGmi-qW"
      },
      "outputs": [],
      "source": [
        "#pandas (Data manipulation, filtering, grouping, merging, pivoting, missing values handling)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"data.csv\")  # Read CSV file\n",
        "df = df.dropna()  # Remove missing values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy (Numerical operations, arrays, missing values, mathematical functions)\n",
        "\n",
        "import numpy as np\n",
        "arr = np.array([1, 2, 3, np.nan])  # Create an array with missing value\n",
        "mean_value = np.nanmean(arr)  # Calculate mean ignoring NaN"
      ],
      "metadata": {
        "id": "0Jm_2DaZjP2T"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# requests (Fetching API data from RESTful web services)\n",
        "\n",
        "import requests\n",
        "response = requests.get(\"https://api.example.com/data\")\n",
        "data = response.json()  # Convert JSON response to Python dictionary"
      ],
      "metadata": {
        "id": "8uJ-gxI8jP4u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json (Parsing JSON responses from APIs and files)\n",
        "\n",
        "import json\n",
        "json_data = '{\"name\": \"Alice\", \"age\": 25}'\n",
        "parsed_data = json.loads(json_data)  # Convert JSON string to dictionary"
      ],
      "metadata": {
        "id": "x3URVCmAjP7D"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xml.etree.ElementTree (Parsing and extracting data from XML files)\n",
        "\n",
        "import xml.etree.ElementTree as ET\n",
        "tree = ET.parse(\"data.xml\")\n",
        "root = tree.getroot()"
      ],
      "metadata": {
        "id": "Q8HEoNubjP-b"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sqlite3 (Working with SQLite databases directly in Python)\n",
        "\n",
        "import sqlite3\n",
        "conn = sqlite3.connect(\"database.db\")\n",
        "df = pd.read_sql_query(\"SELECT * FROM users\", conn)"
      ],
      "metadata": {
        "id": "xCXwXCFTjd7c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sqlalchemy (Connecting Python with databases like MySQL, PostgreSQL, SQLite)\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine(\"sqlite:///data.db\")\n",
        "df.to_sql(\"table_name\", con=engine, if_exists=\"replace\", index=False)"
      ],
      "metadata": {
        "id": "zvT8nPV3jd-z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# schedule (Automating API calls and data fetching at intervals)\n",
        "\n",
        "import schedule\n",
        "import time\n",
        "def job():\n",
        "    print(\"Fetching data...\")\n",
        "schedule.every(10).minutes.do(job)\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "AfB9nlRbjj_6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is API Data Wrangling?\n",
        "\n",
        "API data wrangling refers to the process of fetching, cleaning, transforming, and structuring data from APIs to make it useful for analysis. This involves:\n",
        "\n",
        "✅ Connecting to APIs (REST, GraphQL, SOAP, etc.)\n",
        "\n",
        "✅ Handling Authentication (API keys, OAuth, Bearer tokens)\n",
        "\n",
        "✅ Extracting Data (JSON, XML, CSV responses)\n",
        "\n",
        "✅ Cleaning & Transforming (Filtering, structuring, merging)\n",
        "\n",
        "✅ Storing the Data (DataFrames, databases, CSV, etc.)"
      ],
      "metadata": {
        "id": "QxMOhkoij0dN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BASIC STEPS TO FOLLOW\n",
        "# Install Required Libraries\n",
        "!pip install requests pandas json\n",
        "\n",
        "# Fetch Data from an API\n",
        "import requests\n",
        "\n",
        "# API URL (Replace 'YOUR_API_KEY' with a real API key)\n",
        "url = \"https://api.openweathermap.org/data/2.5/weather?q=Toronto&appid=YOUR_API_KEY\"\n",
        "\n",
        "# Send a GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if request was successful\n",
        "if response.status_code == 200:\n",
        "    data = response.json()  # Convert JSON response to Python dictionary\n",
        "    print(data)\n",
        "else:\n",
        "    print(\"Error:\", response.status_code)\n",
        "\n",
        "# Wrangle API Data Using Pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Extract necessary fields\n",
        "weather_info = {\n",
        "    \"City\": data[\"name\"],\n",
        "    \"Temperature\": data[\"main\"][\"temp\"],\n",
        "    \"Weather\": data[\"weather\"][0][\"description\"],\n",
        "    \"Humidity\": data[\"main\"][\"humidity\"]\n",
        "}\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame([weather_info])\n",
        "print(df)\n",
        "\n",
        "# Handling Pagination (Multiple Pages)\n",
        "base_url = \"https://api.example.com/data?page=\"\n",
        "all_data = []\n",
        "\n",
        "# Loop through first 5 pages\n",
        "for page in range(1, 6):\n",
        "    response = requests.get(base_url + str(page))\n",
        "    if response.status_code == 200:\n",
        "        page_data = response.json()\n",
        "        all_data.extend(page_data[\"results\"])  # Append to list\n",
        "    else:\n",
        "        break  # Stop if API fails\n",
        "\n",
        "df = pd.DataFrame(all_data)\n",
        "print(df.head())\n",
        "\n",
        "# Storing API Data\n",
        "df.to_csv(\"api_data.csv\", index=False) #csv\n",
        "df.to_json(\"api_data.json\", orient=\"records\") #json\n",
        "\n",
        "#to sql\n",
        "from sqlalchemy import create_engine\n",
        "engine = create_engine(\"sqlite:///api_data.db\")\n",
        "df.to_sql(\"weather\", con=engine, if_exists=\"replace\", index=False)\n",
        "\n",
        "\n",
        "# Automating API Data Collection\n",
        "import schedule\n",
        "import time\n",
        "\n",
        "def fetch_weather():\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        print(\"Fetched API Data:\", response.json())\n",
        "\n",
        "schedule.every(1).hour.do(fetch_weather)\n",
        "\n",
        "while True:\n",
        "    schedule.run_pending()\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "hQw7hJ2mjvOA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "lat, lon = 51.5074, -0.1278  # London\n",
        "url = f\"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true\"\n",
        "\n",
        "from geopy.geocoders import Nominatim\n",
        "location = Nominatim(user_agent=\"geoapi\").reverse((51.5074, -0.1278)).address\n",
        "\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    weather = data[\"current_weather\"]\n",
        "    print(f\" Location: {location}\")\n",
        "    print(f\" Temperature: {weather['temperature']}°C\")\n",
        "    print(f\" Wind Speed: {weather['windspeed']} km/h\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTclyjMJXZ46",
        "outputId": "e26cbf7f-dddc-4585-b947-29a93e5c0758"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Location: [object Object], Charing Cross, Seven Dials, Waterloo, City of Westminster, Greater London, England, WC2N 5DX, United Kingdom\n",
            " Temperature: 20.7°C\n",
            " Wind Speed: 13.5 km/h\n"
          ]
        }
      ]
    }
  ]
}